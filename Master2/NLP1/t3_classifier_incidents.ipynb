{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d53d4c2",
   "metadata": {},
   "source": [
    "# Tâche 3 - Classification automatique de descriptions d'incidents\n",
    "\n",
    "Cette tâche vise à classifier de courts textes décrivant des incidents qui se sont produits sur des chantiers de construction. Pour chaque incident, on retrouve une étiquette de 1 à 4. Ces étiquettes sont inconnues et vous devrez tenter de les identifier à la section 3 de ce *notebook*. \n",
    "\n",
    "Les objectifs de cette tâche sont: \n",
    "- de se familiariser avec la classification de texte\n",
    "- d'apprendre à utiliser les fonctions de base de scikit-learn\n",
    "- de comprendre comment représenter un texte sous la forme d'un sac de mots (*bag of words*)\n",
    "- de faire l'évaluation d'un modèle de classification avec un corpus de test\n",
    "- de tenter d'interpréter les résultats d'un modèle à l'aide des poids d'attributs. \n",
    "\n",
    "Pour la première partie, vous devez construire une fonction (*train_and_test_classifier*) qui entraîne un modèle (les options étant la régression logistique et le naïf bayésien) et en faire l'évaluation sur des données d'entraînement et des données de test. Deux fichiers de textes sont disponibles pour mener votre expérimentation (voir Section 1). \n",
    "\n",
    "Pour la deuxième partie, vous devez tentez de déterminer à quoi correspond chacune des classes d’incident. Faites une analyse des poids des modèles pour proposer des étiquettes pour chacune des classes. Vous pouvez vous inspirer des *notebooks* disponibles sur le site du cours. Expliquez clairement comment vous êtes arrivé à vos conclusions. \n",
    "\n",
    "Merci de respecter les signatures des fonctions *train_and_test_classifier* et *load_incident_dataset*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cb0b9",
   "metadata": {},
   "source": [
    "## Section 1 - Lecture des fichiers de données\n",
    "\n",
    "Voici les fichiers mis à votre disposition pour mener vos expérimentations. La fonction *load_incident_data* peut être utilisée pour lire les 2 fichiers (train et test). Rien à modifier dans cette section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7082859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_json_fn = \"./data/t3_train.json\"\n",
    "test_json_fn = \"./data/t3_test.json\"\n",
    "\n",
    "\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "    return incident_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aea833",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = load_incident_dataset(train_json_fn)\n",
    "print(\"Nombre d'incidents:\", len(train_list))\n",
    "print(\"\\nUn exemple:\\n\", train_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88755bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = load_incident_dataset(test_json_fn)\n",
    "print(\"Nombre d'incidents\", len(test_list))\n",
    "incident = test_list[10]\n",
    "print(\"\\nUne description d'incident:\", incident[\"text\"])\n",
    "print(\"\\nSon étiquette:\", incident[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28551b1",
   "metadata": {},
   "source": [
    "## Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a2469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a418049",
   "metadata": {},
   "source": [
    "## Section 2 - Entraînement et évaluation des modèles  \n",
    "\n",
    "Vous pouvez ajouter tout le code dont vous avez besoin pour l'entraînement. Merci de ne pas modifier la signature de la fonction d'entraînement et de bien expliquer votre démarche et vos résultats. N'oubliez pas de faire une recommandation de modèle. Vous pouvez ajouter des cellules au *anotebook* si nécessaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0b1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du sac de mots\n",
    "def get_bows(train_text_set, test_text_set):\n",
    "    \"\"\"\n",
    "    Vectorise un ensemble de phrases au moyen de la technique Bag of Words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_text_set: list of dict: liste contenant un dictionnaire par\n",
    "                                  texte d'entraînement de la forme suivante:\n",
    "                                  {\"text\": str, \"label\": int}\n",
    "    test_text_set: list of dict: liste contenant un dictionnaire par\n",
    "                                 texte de test de la forme suivante:\n",
    "                                 {\"text\": str, \"label\": int}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_bow: np array: \"sac de mots\" des données d'entraînement, tableau numpy\n",
    "    avec un vecteur par phrase de forme (1, nombre de mots dans le vocabulaire)\n",
    "    test_bow: np array: \"sac de mots\" des données de test, tableau numpy avec\n",
    "    un vecteur par phrase de forme (1, nombre de mots dans le vocabulaire)\n",
    "    \n",
    "    \"\"\"\n",
    "    # -- Récupère les textes des 2 jeux de données dans des listes\n",
    "    train_text_corpus = [text[\"text\"].strip() for text in train_text_set]\n",
    "    test_text_corpus = [text[\"text\"].strip() for text in test_text_set]\n",
    "\n",
    "    # -- Initialise une instance du CountVectorizer de sklearn qui permet de vectoriser\n",
    "    # -- un ensemble de phrases selon la méthode Bag of Words\n",
    "    vectorizer = CountVectorizer(lowercase=True, max_df=0.85, max_features=270)\n",
    "\n",
    "    # -- Entraîne le vectorizer et transforme le corpus d'entraînement en vecteurs\n",
    "    train_bow = vectorizer.fit_transform(train_text_corpus)\n",
    "    test_bow = vectorizer.transform(test_text_corpus)\n",
    "    \n",
    "    # -- Transforme les matrices scipy en matrices numpy\n",
    "    train_bow = train_bow.toarray()\n",
    "    test_bow = test_bow.toarray()\n",
    "\n",
    "    # -- Construit un dataframe avec les mots présents dans le vectorizer\n",
    "    df = pd.DataFrame(vectorizer.get_feature_names_out(), columns =['Mots'])\n",
    "    \n",
    "    return train_bow, test_bow, df\n",
    "\n",
    "def get_labels(text_set):\n",
    "    \"\"\"\n",
    "    Petite fonction utilitaire pour récupérer les labels dans un vecteur\n",
    "    numpy compatible pour le Naïve Bayes Classifier de sklearn.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_set: list of dict: liste contenant un dictionnaire par\n",
    "                            texte de la forme suivante:\n",
    "                            {\"text\": str, \"label\": int}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels: numpy array: vecteur numpy du format (nombre d'éléments dans text_set, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # -- Récupérer tous les labels dans une liste\n",
    "    labels_list = [text[\"label\"] for text in text_set]\n",
    "    # -- Convertir la liste en tableau numpy et transposer le vecteur obtenu\n",
    "    # -- pour correspondre au format du classificateur de sklearn\n",
    "    labels = np.transpose(np.array(labels_list))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def train_and_test_classifier(train_fn, test_fn, model='NB'):\n",
    "    \"\"\"\n",
    "    :param train_fn et test_fn: les 2 fichiers utilisées pour entraîner et tester les classificateurs.\n",
    "    :param model: le type de classificateur. NB = Naive Bayes, LR = Régression logistique.\n",
    "    :return: un dictionnaire contenant 3 valeurs:\n",
    "                 - l'accuracy à l'entraînement (validation croisée)\n",
    "                 - l'accuracy sur le jeu de test\n",
    "                 - la matrice de confusion calculée par scikit-learn sur les données de test\n",
    "    \"\"\"\n",
    "    # Récupération des sacs de mots des 2 jeux de données\n",
    "    train_bow, test_bow, df = get_bows(train_fn, test_fn)\n",
    "    # Récupération des labels d'entraînements dans un vecteur cible \"y\"\n",
    "    y_train = get_labels(train_fn)\n",
    "    y_test = get_labels(test_fn)\n",
    "\n",
    "    # Initialisation et entraînement du classificateur\n",
    "    if model == \"NB\": # Si le modèle souhaité est le Naïve Bayes Classifier\n",
    "        clf = MultinomialNB(alpha=0.01)\n",
    "        clf.fit(train_bow, y_train)\n",
    "        for i in range(len(clf.classes_)):\n",
    "            ## Pour chaque classe ajoute au dataframe de mot la log probabilité\n",
    "            ## que le mot se trouve dans la classe\n",
    "            df[clf.classes_[i]] = list(clf.feature_log_prob_[i])\n",
    "\n",
    "    if model == \"LR\": \n",
    "        clf = LogisticRegression(random_state=0)\n",
    "        clf.fit(train_bow, y_train)\n",
    "        for i in range(len(clf.classes_)):\n",
    "            ## Pour chaque classe ajoute au dataframe de mot la log probabilité\n",
    "            ## que le mot se trouve dans la classe\n",
    "            df[clf.classes_[i]] = list(clf.coef_[i])\n",
    "\n",
    "    ## -- Trie le dataframe de la log prob la plus élevée à la plus basse\n",
    "    ## -- pour que les mots les plus probables apparaîssent en premier lieu\n",
    "    for i in range(0,4):\n",
    "        sorted_df = df[[\"Mots\", clf.classes_[i]]].sort_values(\n",
    "                by=[clf.classes_[i]], ascending=False, ignore_index=True\n",
    "            )\n",
    "        ## Liste des termes qui ne donne pas d'indication sur la nature de la classe\n",
    "        unpertinent_words = ['how', 'what', 'when', 'where', 'which', 'who',\n",
    "                             'whom', 'whose', 'why', 'the', 'is', 'are',\n",
    "                             'for', 'the', 'each', 'some', 'did', 'to', 'that',\n",
    "                             'had', 'as', 'it', 'while', 'into', 'of', 'he', 'in',\n",
    "                             'at', 'his', 'were', 'from', 'an', 'by', 'with',\n",
    "                             'not']\n",
    "        ## Affiche les 20 mots qui ont le plus de chance de se trouver dans\n",
    "        ## la classe i et donc les plus représentatifs\n",
    "        print(\"MOTS + PROBABLES DE LA CLASSE %d EN FONCTION\\nDES COEFFICIENTS DU MODÉLE\"%clf.classes_[i])\n",
    "        print(\"--------------------------------------------\")\n",
    "        display(\n",
    "            sorted_df[~sorted_df[\"Mots\"].isin(unpertinent_words)][:20]\n",
    "        )\n",
    "   \n",
    "    # -- Evaluation sur les données d'entraînement\n",
    "    train_scores = cross_val_score(clf, train_bow, y_train, cv=10) # Validation croisée\n",
    "    mean_train_accuracy = train_scores.mean()                      # Moyenne des exactitudes obtenues à chaque validation\n",
    "\n",
    "    # -- Evaluation sur les données de test\n",
    "    y_pred = clf.predict(test_bow) \n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Les résultats à retourner\n",
    "    results = dict()\n",
    "    results['accuracy_train'] = mean_train_accuracy\n",
    "    results['accuracy_test'] = test_accuracy\n",
    "    results['confusion_matrix'] = conf_matrix\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2539251-a6b1-4069-8691-e29a38506d98",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Classification avec le Classificateur Naïf Bayésien </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409602f-3a0f-497b-921b-d12e3936be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nb = train_and_test_classifier(train_list, test_list, model='NB')\n",
    "results_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4829ef-2f1d-4940-80fe-a11e2f00e50c",
   "metadata": {},
   "source": [
    "#### Commentaire des Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b43ce-95c5-401b-bb4b-1a3635282aea",
   "metadata": {},
   "source": [
    "Avec le vectorizer et le classificateur de base, les résultats n'étaient pas excellents (autour des 65%). Nous avons donc joué avec les différents paramètres pour améliorer la classification: \n",
    "\n",
    "* Tout d'abord, le paramètre \"lowercase\" fixé à True permet un prétraitement du texte en mettant tous les termes en minuscules. Ça permet d'éviter les doublons. Cette manipulation n'a pas augmenté les résultats de manière signifactive, mais c'est une bonne pratique de prétraitement selon nous (sauf peut-être pour les tâches de reconnaissance d'entités nommées pour lesquelles la majuscule est un bon indicateur). \n",
    "\n",
    "* Ensuite, nous avons diminué \"max_df\" initialement fixé à 1. Ce paramètre définit le seuil au-dessus duquel les termes ne doivent pas être gardés dans le vocabulaire. Autrement dit, les termes présent dans plus de 85% des documents du corpus en l'occurence ne seront pas gardés. Grâce à ça, le score a augmenté de manière drastique (jusqu'à 85% environ), ce qui est logique car les termes présents dans beaucoup de documents sont sûrement très communs et peu informatifs sur la nature du document. En les éliminant, on réduit le nombre de variables à prendre en compte dans le classificateurs et celui-ci s'en sort mieux. \n",
    "\n",
    "* Enfin, la dernière manipulation pour la vectorisation qui nous a fait atteindre les 95% environ est le passage du paramètre *max_feature*, initié à 270, au vectorizer. Ce paramètre fait en sorte que le vectorizer ne garde que les 270 meilleurs token en terme de \"term frequency\", c'est-à-dire les mots qui apparaissent le plus par document et donc qui sont les plus informatifs. \n",
    "\n",
    "* Nous avons également joué sur le paramètre *alpha* dans le classificateur en le fixant à 0.01. Ce dernier assignera aux termes qui n'apparaissent pas dans une classe donnée une probabilité de 0,01 d'apparition dans cette classe pour éviter d'avoir des probabilités de classe de 0. Cette dernière manipulation a permis d'augmenter encore légèrement les résultats en entraînement et en test avec respectivement **96,18%** et **97,04%** d'exactitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251611f-bfa6-4209-99ea-737a6f24b1c8",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Classification avec la Régression Logistique </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d1778-9ee7-4ee9-bfc3-db2d613ee68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lr = train_and_test_classifier(train_list, test_list, model='LR')\n",
    "results_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2e1b9-4ab1-44bd-a4b1-5b8fdcd32f19",
   "metadata": {},
   "source": [
    "#### Commentaire des Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3c352-b3d1-4403-a527-bc1a4578c8eb",
   "metadata": {},
   "source": [
    "Ici, seuls les paramètres du vectorizer ont été modifiés. A elles seules, ces modifications ont permis au modèle d'atteindre une exactitude de 99,24% en entraînement et de 99,43% en test. Les résultats de la régression logistique sont meilleurs que ceux du classificateur naïf bayésien car elle permet de modéliser des relations plus complexes entre les variables et est donc plus adaptée pour ce genre de tâche. Cependant, de manière générale les résultats sont très élevés et une dernière raison peut être avancer pour justifier cela. Celle-ci est mentionnée dans la section suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9dbd2",
   "metadata": {},
   "source": [
    "## Section 3 - À quoi correspondent les classes? Explicabilité du modèle\n",
    "\n",
    "En utilisant les poids des modèles, tentez d'attribuer une signification aux différentes classes. Il devrait être possible de définir précisément la nature et la composition des classes. L'important est d'utiliser ce qu'on observe dans les modèles pour fournir une explication plausible.\n",
    "\n",
    "Vous pouvez ajouter tout le code et toutes les cellules dont vous avez besoin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2bf6c-2a6b-4008-a952-80238a2728db",
   "metadata": {},
   "source": [
    "*Tout le code utilisé pour montrer les coefficients se trouve dans la fonction de la Section 2 pour faciliter l'implémentation*\n",
    "\n",
    " Comme le montre les dataframes des coefficients en Section 2, les mots pertinents qui diffèrent d'une classe à l'autre sont uniquement **les mois**. On peut donc en déduire que chaque classe répertorie les incidents survenus sur le chantier lors d'un **trimestre en particulier**. \n",
    " \n",
    "  * La classe 1 répertorie les incidents de janvier-février-mars \n",
    "  * La 2 ceux d'avril-mai-juin \n",
    "  * La 3 ceux de juillet-août-septembre\n",
    "  * La 4 ceux d'octobre-novembre-décembre. \n",
    "  \n",
    "  Ça nous donne également des explications quant aux résultats extrêment élevés des deux classificateurs. En effet, puisque la nature des textes est très similaire, mais que seuls les mois changent, le nom du mois est un excellent indicateur du label à assigner. Combiné au fait que la taille du jeu de données d'entraînement est élevée par rapport à la facilité de la tâche, les modèles peuvent facilement distinguer un pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd9d7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40076487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ffcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706eba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8518aab0",
   "metadata": {},
   "source": [
    "## Section 4 - Section réservée pour nos tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b8bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ab13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811c1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f9746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1912bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
